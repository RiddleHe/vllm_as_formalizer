{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19b47910",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json, re\n",
    "from collections import defaultdict, OrderedDict\n",
    "from statistics import mean, stdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75bc7e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = Path(\"/home/mh3897/vllm_as_formalizer/results/precision_recall\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158606bd",
   "metadata": {},
   "source": [
    "## Main table: success rate across methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "378fc03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MULTI_RUN_DATASETS = {\"blocksworld-small\", \"cooking-small\"}\n",
    "METRIC_KEYS = [\n",
    "    \"simulation_success_rate\",\n",
    "    \"plan_success_rate\",\n",
    "    \"compilation_success_rate\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53d68688",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_metrics_from_file(path):\n",
    "    with path.open(\"r\") as f:\n",
    "        data = json.load(f)\n",
    "    summary = data.get(\"summary\", {})\n",
    "    counts = summary.get(\"counts\", {})\n",
    "\n",
    "    tasks_total = summary.get(\"tasks_total\", 0)\n",
    "    successes = summary.get(\"successes\", 0)\n",
    "    tasks_with_plan = summary.get(\"tasks_with_plan\", 0)\n",
    "    parse_errors = counts.get(\"task_with_parse_error\", 0)\n",
    "\n",
    "    if not tasks_total:\n",
    "        return {k: float(\"nan\") for k in METRIC_KEYS}\n",
    "\n",
    "    return {\n",
    "        \"simulation_success_rate\": successes / tasks_total,\n",
    "        \"plan_success_rate\": tasks_with_plan / tasks_total,\n",
    "        \"compilation_success_rate\": (tasks_total - parse_errors) / tasks_total,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0eed5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_filename(path):\n",
    "    parts = path.stem.split(\"-\")\n",
    "    \n",
    "    if parts[0] == \"gpt\" and parts[1].startswith(\"4.\"):\n",
    "        model = \"-\".join(parts[:2])\n",
    "        rest = parts[2:]\n",
    "    else:\n",
    "        model = parts[0]\n",
    "        rest = parts[1:]\n",
    "    \n",
    "    if rest[0] in {\"blocksworld\", \"cooking\"}:\n",
    "        dataset = \"-\".join(rest[:2])\n",
    "        rest = rest[2:]\n",
    "    else:\n",
    "        dataset = rest[0]\n",
    "        rest = rest[1:]\n",
    "\n",
    "    run = None\n",
    "    if rest and rest[-1].isdigit() and len(rest) >= 2 and rest[-2] == \"run\":\n",
    "        run = int(rest[-1])\n",
    "        rest = rest[:-2]\n",
    "    \n",
    "    pipeline = \"-\".join(rest)\n",
    "\n",
    "    return model, dataset, pipeline, run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "351e234b",
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_by = defaultdict(lambda: defaultdict(lambda: defaultdict(list)))\n",
    "\n",
    "for path in sorted(BASE_DIR.glob(\"*.json\")):\n",
    "    try:\n",
    "        model, dataset, pipeline, run = parse_filename(path)\n",
    "    except Exception:\n",
    "        continue\n",
    "    runs_by[model][dataset][pipeline].append(load_metrics_from_file(path))\n",
    "\n",
    "def mean_of_list(dlist, key):\n",
    "    return mean(d[key] for d in dlist)\n",
    "\n",
    "def std_of_list(dlist, key):\n",
    "    vals = [d[key] for d in dlist]\n",
    "    return stdev(vals) if len(vals) > 1 else 0\n",
    "\n",
    "result = defaultdict(dict)\n",
    "\n",
    "for model, ds_map in runs_by.items():\n",
    "    for dataset, pipeline_map in ds_map.items():\n",
    "        for pipeline, runs in pipeline_map.items():\n",
    "            mean_metrics = {k: mean_of_list(runs, k) for k in METRIC_KEYS}\n",
    "            ds_entry = {\"mean\": mean_metrics}\n",
    "\n",
    "            if dataset in MULTI_RUN_DATASETS and len(runs) > 1:\n",
    "                std_metrics = {k: std_of_list(runs, k) for k in METRIC_KEYS}\n",
    "                ds_entry[\"std\"] = std_metrics\n",
    "\n",
    "            result[model].setdefault(dataset, {})[pipeline] = ds_entry    \n",
    "\n",
    "def round_nested(obj, ndigits=4):\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: round_nested(v, ndigits) for k, v in obj.items()}\n",
    "    if isinstance(obj, list):\n",
    "        return [round_nested(v, ndigits) for v in obj]\n",
    "    if isinstance(obj, float):\n",
    "        return round(obj, ndigits)\n",
    "    return obj\n",
    "\n",
    "result_rounded = round_nested(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e00d5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = Path(\"/home/mh3897/vllm_as_formalizer/results/findings\")\n",
    "OUTPUT_PATH = OUTPUT_DIR / \"success_rates.json\"\n",
    "with OUTPUT_PATH.open(\"w\") as f:\n",
    "    json.dump(result_rounded, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c299ba",
   "metadata": {},
   "source": [
    "## Table for precision and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e98af512",
   "metadata": {},
   "outputs": [],
   "source": [
    "SECTIONS = (\"objects\", \"init\", \"goal\")\n",
    "SKIP_PIPELINES = {\"direct-plan\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0506fde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_run = defaultdict(list)\n",
    "\n",
    "for path in sorted(BASE_DIR.glob(\"*.json\")):\n",
    "    model, dataset, pipeline, run = parse_filename(path)\n",
    "    if pipeline in SKIP_PIPELINES:\n",
    "        continue\n",
    "    with path.open() as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    by_section = data.get(\"summary\", {}).get(\"by_section\", {})\n",
    "    for s in SECTIONS:\n",
    "        sec = by_section.get(s, {})\n",
    "        p = float(sec.get(\"macro_precision\", 0.0))\n",
    "        r = float(sec.get(\"macro_recall\", 0.0))\n",
    "        \n",
    "        per_run[(model, pipeline, dataset, s)].append((float(p), float(r)))\n",
    "\n",
    "per_ds = {}\n",
    "\n",
    "for key, pr_list in per_run.items():\n",
    "    model, pipe, ds, sec = key\n",
    "    ps = [p for p, _ in pr_list]\n",
    "    rs = [r for _, r in pr_list]\n",
    "    per_ds[key] = (mean(ps) if ps else 0.0, mean(rs) if rs else 0.0)\n",
    "\n",
    "def f1_from_pr(p, r):\n",
    "    return (2*p*r / (p+r)) if (p+r) else 0.0\n",
    "\n",
    "per_model_pipe = defaultdict(\n",
    "    lambda: {s: {\"precisions\": [], \"recalls\": []} for s in SECTIONS}\n",
    ")\n",
    "for (model, pipeline, dataset, section), (p, r) in per_ds.items():\n",
    "    per_model_pipe[(model, pipeline)][section][\"precisions\"].append(p)\n",
    "    per_model_pipe[(model, pipeline)][section][\"recalls\"].append(r)\n",
    "\n",
    "result_macro = {}\n",
    "\n",
    "for (model, pipe), sec_vals in per_model_pipe.items():\n",
    "    result_macro.setdefault(model, {})\n",
    "    result_macro[model].setdefault(pipe, {})\n",
    "    for s in SECTIONS:\n",
    "        avg_p = mean(sec_vals[s][\"precisions\"]) if sec_vals[s][\"precisions\"] else 0.0\n",
    "        avg_r = mean(sec_vals[s][\"recalls\"]) if sec_vals[s][\"recalls\"] else 0.0\n",
    "        result_macro[model][pipe][s] = {\n",
    "            \"precision\": avg_p,\n",
    "            \"recall\": avg_r,\n",
    "            \"f1\": f1_from_pr(avg_p, avg_r),\n",
    "        }\n",
    "result_macro_rounded = round_nested(result_macro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e35a9bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH_PR = OUTPUT_DIR / \"precision_recall.json\"\n",
    "with OUTPUT_PATH_PR.open(\"w\") as f:\n",
    "    json.dump(result_macro_rounded, f, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "villain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
